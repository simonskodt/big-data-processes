{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71575219",
   "metadata": {},
   "source": [
    "# Big Data Processes Exercises - Week 06\n",
    "# <font color= MediumSpringGreen>CodeCarbon</font>\n",
    "\n",
    "#### What we will cover today\n",
    "\n",
    "<ol>\n",
    "    <li>Importing packages and libraries</li>\n",
    "    <li>Loading the dataset</li>\n",
    "    <li>CodeCarbon</li>\n",
    "    <ol>\n",
    "        <li>Decision Tree from week 3</li>\n",
    "        <li>Testing with CodeCarbon</li>\n",
    "        <li>Evaluating the model</li>\n",
    "        <li>Evaluating emissions</li>\n",
    "    </ol>\n",
    "</ol>\n",
    "\n",
    "Info about CodeCarbon: https://mlco2.github.io/codecarbon/\n",
    "\n",
    "***\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a70e9f",
   "metadata": {},
   "source": [
    "## 1. Importing various libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc57655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install seaborn\n",
    "#%pip install sklearn\n",
    "#pip install scikit-learn\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f31c09a",
   "metadata": {},
   "source": [
    "### 1.1 Installing and importing <font color=MediumSpringGreen>CodeCarbon </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a231297",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install codecarbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019249b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from codecarbon import EmissionsTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edac3fcc",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## 2. Load and examine the data\n",
    "\n",
    "Yet again, we will again be using the **IBM-employee-attrition dataset** where we will try and predict if an employee has attrition or not, aka. whether they have left the company or not. \n",
    "\n",
    "As we have explained before, our target variable, attrition, is either be 0 or 1:\n",
    "- 0 = No attrition, the employee did not leave the company. The negative class\n",
    "- 1 = Attrition, the employee left the company. The positive class <-- our focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d231678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"IBM-Employee-Attrition.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56236a6",
   "metadata": {},
   "source": [
    "Examine the notebook if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb41d730",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5eeede",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 3. Classification Decision Trees with an EmissionsTracker\n",
    "\n",
    "We will try CodeCarbon on the Classification Decision Tree model week 3 (Classification) in order to test how many emissions does our model release "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9f6b5a",
   "metadata": {},
   "source": [
    "### 3.1 Selecting target features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47718d6",
   "metadata": {},
   "source": [
    "We will select the same features as we've used before for our models (go back to the notebook for week 3, if you need a refresher as to how we got these features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4042fd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the feature and target variables\n",
    "#From list of feature(s) 'X', the model will guess/predict the 'y' feature (our target)\n",
    "X = df[['EnvironmentSatisfaction', 'JobSatisfaction', 'JobInvolvement', 'YearsAtCompany', 'StockOptionLevel', 'YearsWithCurrManager', 'Age', 'MonthlyIncome', 'YearsInCurrentRole', 'JobLevel', 'TotalWorkingYears']].values\n",
    "\n",
    "y = df['Attrition'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c088e6",
   "metadata": {},
   "source": [
    "### 3.2 Split the data in training and test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c8f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9b635c",
   "metadata": {},
   "source": [
    "No need to standardise this time since we are working with a Decision Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1b3f1b",
   "metadata": {},
   "source": [
    "### 3.3 Create the model and initialise the EmissionsTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf555c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier with some hyperparameter tuning from week 4\n",
    "model_DTC = DecisionTreeClassifier(max_depth=5, min_samples_leaf=2, min_samples_split=5, random_state=42)\n",
    "\n",
    "tracker = EmissionsTracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3c76f2",
   "metadata": {},
   "source": [
    "### 3.4 Start the tracker and then fit/train the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82e7665",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start tracking carbon emissions\n",
    "tracker.start()\n",
    "\n",
    "# fit the classifier to the standardized training data\n",
    "model_DTC.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772022aa",
   "metadata": {},
   "source": [
    "###  3.5 Make prediction and stop the tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c57b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model_DTC.predict(X_test)\n",
    "\n",
    "# Stop tracking carbon emissions\n",
    "tracker.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9820265a",
   "metadata": {},
   "source": [
    "### 3.6 Evaluate the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a16598",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We evaluate the performance of the classifier using the accuracy score\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269cd640",
   "metadata": {},
   "source": [
    "~0.82 = 82% Accuracy - that can be pretty good for a model. But as we learned last week, evaluating on 'Accuracy' does not work on our imbalanced dataset. We need to use other evaluation metrics.\n",
    "\n",
    "#### 3.6.1 Other evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46f60f6",
   "metadata": {},
   "source": [
    "There are many ways to evaluate machine learning models - what is important to note, is to figure out which evaluation metric or score is best for your <font color=red> model and data </font>. If you need a recap of the different evaluation metrics, we have written a wonderful look-up section in the last week's Notebook 'BDP_Evaluation.ipynb'.\n",
    "\n",
    "Last week, we found out that our IBM-Employee-Attrition dataset is imbalanced. Meaning, when we try and predict the 'Attrition' class, the class is very small compared to the 'No Attrition' majority class. Therefore we cannot use the accuracy metric anymore, since we know that our model will seldom predict an instance of 'Attrition' since it hasn't learned enough about the 'Attrition' class in order for it to predict it. \n",
    "\n",
    "Instead, we will use the *precision*, *recall* and *f1* scores. \n",
    "- Precision summarizes the fraction of examples assigned the positive class that belong to the positive class. (aka. who had attrition that was correctly predicted for attrition)\n",
    "- Recall summarizes how well the positive class was predicted. (aka. how well the 'Attrition' instances were predicted)\n",
    "- F1 score combines both precision and recall into a single score, that balances both scores.\n",
    "\n",
    "For our case, we are equally instereted in False Negatives and False Positives, aka. in the wrongs of our model, so we will focus on the F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9744f9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_test, y_pred) \n",
    "# Recall \n",
    "recall = recall_score(y_test, y_pred) \n",
    "# F1-Score \n",
    "f1 = f1_score(y_test, y_pred) \n",
    "\n",
    "print(\"Precision:\", precision) \n",
    "print(\"Recall:\", recall) \n",
    "print(\"F1-Score:\", f1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09d486c",
   "metadata": {},
   "source": [
    "Okay so this is not the greatest model - but what is most important, how much CO2 did our model emit? Hint: look at the output printed when the tracker was stopped..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002a6767",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 4. Emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997b6781",
   "metadata": {},
   "source": [
    "After ending the EmissionsTracker, it will save a dataframe as a .csv-file in your directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e142a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_df = pd.read_csv(\"emissions.csv\")\n",
    "emissions_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cc30c7",
   "metadata": {},
   "source": [
    "You can read more about the columns, what they represent and their format, at: https://mlco2.github.io/codecarbon/output.html#csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec859e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is a way to display all the column in one dataframe\n",
    "pd.set_option('display.max_columns', None) \n",
    "emissions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a458cb",
   "metadata": {},
   "source": [
    "N.B.: the data from your code is saved in the last row of the dataframe. \n",
    "The other rows represent previous measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b162b26",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef61d9f",
   "metadata": {},
   "source": [
    "## Your turn ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d82cbf3",
   "metadata": {},
   "source": [
    "#### We encourage you to try the EmissionsTracker on models in your own project or on models from previous exercise sessions. Try out different classification models, with/without hyperparameter tuning, run your model on different computers, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4788d569",
   "metadata": {},
   "source": [
    "Link to the Quickstart guide for CodeCarbon.io https://mlco2.github.io/codecarbon/usage.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
