{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Processes Exercises - Week 07\n",
    "\n",
    "## <font color = pink>Data cleaning  & Exploratory Data Analysis</font>\n",
    "\n",
    "#### What we will cover today:\n",
    "\n",
    "1. Importing packages and libraries\n",
    "2. Loading and examining the dataset\n",
    "3. Exploratory Data Analysis\n",
    "4. Removing unnecessary columns\n",
    "5. Data cleaning\n",
    "6. Pearson\n",
    "    1. Heatmap\n",
    "7. Distribution and Visualisations\n",
    "8. Outliers\n",
    "9. Features with biggest impact\n",
    "\n",
    "***\n",
    "***\n",
    "***\n",
    "\n",
    "## 1. Importing various libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from numpy import asarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## 2. Loading the dataset\n",
    "\n",
    "We will be working with the IBM-Employee-Attrition dataset yet again. However this time, **<font color=Orchid>we will be working with the <i>'raw'</i>, original dataset </font>** - Straight Outta Kaggle.\n",
    "\n",
    "In the raw dataset, 'Attrition' is actually 'Yes' and 'No' and not '1' and '0'. During this Notebook you will learn how to change such instances into numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"IBM.csv\", delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color= turquoise>EXERCISE 1</font>**\n",
    "Examine the dataset throughly. What are some other differences between this 'raw' dataset and the cleaned IBM-dataset, we have worked with so far? For easier comparison, load the other IBM-Employee-Attrition dataset into the Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "We will be working with the 'Attrition' column - our target variable. We want to figure out whether an employee has Attrition or not, i.e., whether the employee left the company or not.\n",
    "\n",
    "Since we will be working with the 'Attrition' feature, let's start by moving it to the front column in the dataset for better visability.\n",
    "\n",
    "*If you want more information about the columns and the values behind the columns, we refer to the Kaggle page from which this dataset is from https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset/data and https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset/discussion/139552*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We copy the column, drop (i.e., delete) it from our dataframe and insert it to the front\n",
    "front = df['Attrition']\n",
    "df.drop(labels = ['Attrition'], axis=1, inplace=True)\n",
    "df.insert(0, 'Attrition', front)\n",
    "\n",
    "#displaying quick overview for the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Below are examples of bits of code that you can use for initial exploration/getting an overview of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking all unique values\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 4. Removing unnecessary columns (features)\n",
    "\n",
    "Above, we called the nunique() method. As you can see from the output, some columns have few unique values. For instance, 'Attrition' has only two different values ('Yes'  and 'No'). 'EducationField' on the other hand, has five unique values ('Life Sciences', 'Other', 'Medical', 'Marketing', 'Technical Degree', 'Human Resources'). Finally, 'EmployeeNNumber' has 1470 unique values. This is because each employee has their unique employeee number and the dataset has information on 1470 different employees.\n",
    "\n",
    "As you can see from the examples above, the number of unique values vary from column to column. When a column (feature) has many unique and distinct values, it is deemed to have **high cardinality.** \n",
    "\n",
    "- High cardinality is when a feature has many values. A boolean column, which only can have the values of <font color=pink> true or false </font>, has a cardinality of 2. HTTP status codes – <font color=pink> 200, 301, 302, 404, 500 </font> – might have a cardinality under a few dozen. But, for an online shopping system, fields like <font color=pink> userId, shoppingCartId, and orderId </font> are often high-cardinality columns that can take take hundreds of thousands of distinct values. \n",
    "A large number of distinct values, tend to provide more detail for observation. On the other hand, attributes with low cardinality, having only a few distinct values, may limit .the level of insight that can be obtained. However, it is safe to say, that *very* high cardinality does not provide us with better insight or give our model a pattern for it's prediction.\n",
    "\n",
    "Selecting a column and using .unique() method gives us unique values within that variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Attrition'].unique() # cardinality of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EducationField'].unique() # cardinality of 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EmployeeNumber'].unique() # cardinality of 1470 as every employee has their own number (N.B.: some numbers, like 3, are missing from the dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Features with high/low cardinality\n",
    "\n",
    "Let us take a closer look at some of the the columns with a suspiciously high or low number of unique values. The reason we want to look at these is, that it is important to somehow deal with the features with high cardinality - either by transforming the values or completely deleting the column. \n",
    "\n",
    "OBS! Remember to write that in your reports as to why you're deleting and/or transforming your data, as a simple action as that can introduce bias in your model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EmployeeCount'].unique() # this column has only one value: '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MonthlyRate'].unique() # this column has a lot of different values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Over18'].unique() # this column too has only one value: 'Y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['StandardHours'].unique() # this column too has only one value: '80'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EmployeeCount, Over18, and StandardHours** contain only one unique value. \n",
    "\n",
    "\n",
    "- **EmployeeCount:** We cannot find more information about the column EmployeeCount on Kaggle - we can only assume it describes the number of employees represented in a given observation, which won't add anything to our prediction, since it is the same value ('1') across all observations.\n",
    "- **Over18:** Every value in this column is 'Y', meaning the employee is over 18 years of age. This won't add to our prediction of Attrition either.\n",
    "- **StandardHours:** Every value in this column is '80'. The unit here is unclear and no more information on Kaggle is provided.\n",
    "\n",
    "Since all of the abovementioned values are the same, we can confidently remove the columns, as they will be useless in helping us predicting our target.\n",
    "\n",
    "**EmployeeNumber** is a somewhat different story:\n",
    "\n",
    "- **EmployeeNumber:** As described, this column holds the personal number of the given employee. It is probably some kind of ID. This column won't help us predict attrition, since it only contains \"a number describing a person\". In other words, there is no pattern that will emerge from this column that will help us predict attrition. \n",
    "\n",
    "Remember! This is our own assumption! Remember to note and write down everything you do with the dataset and why in your project reports!\n",
    "\n",
    "Also! <font color=pink> *If you want to double-check which columns can be dropped, create a pearson correlation matrix! Columns with 'NaN' are a good indicator that they can be dropped. Hint hint: Notebook 3 BDP_Classification.ipynb. However! This does not apply to columns like 'EmployeeNumber' since it contains multiple unique values and not a single value*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting the columns 'EmployeeNumber', 'EmployeeCount', 'Over18' and 'StandardHours' and saving the result in our variable 'df'\n",
    "df = df.drop(['EmployeeNumber', 'EmployeeCount', 'Over18', 'StandardHours'],  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking that the columns has indeed been deleted\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## 5. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5.1 Handling nominal/categorical data\n",
    "\n",
    "A lot of the features/columns in the IBM dataset contain nominal/categorical data (if you are confused by the terms 'nominal', 'ordinal' etc., take a look in this week's text 'Exploratory Data Analysis' by Victoria Cox). \n",
    "\n",
    "To use nominal data for modeling, we need to transform it to numerical data, more on how to do this in a minute (or take a look here: https://towardsdatascience.com/handling-categorical-data-the-right-way-9d1279956fc6). \n",
    "\n",
    "First, let's look at the different types of nominal data.\n",
    "\n",
    "Nominal data can be:\n",
    "- *non-ordinal*, i.e. categories that are *not* ordered. In other words, the catgories have no natural, internal ordering. An example is a variable such as 'EducationField' which holds the following categories: 'Life Sciences', 'Other', 'Medical', 'Marketing', 'Technical Degree', 'Human Resources'\n",
    "- *ordinal*, i.e. categories that have a natural order. An example *could* be a variable such as 'Education'. Here bachelor is below master which is below phd. However, be aware that this is *our* assumption. In your projects, you should explain the reasons for deciding that a nominal variable contains ordinality. You should also explain your choice of encoding, i.e., why you have chosen to handle the nominal variables in your dataset in the way you have.\n",
    "\n",
    "When we handle nominal/categorical data, we need to assess whether the data is non-ordinal or ordinal, because these two types of data should often be handeled differently. \n",
    "\n",
    "**Binary, nominal data** which hold only two categories (i.e., the binary variables) can be handled using something called dummy coding. Dummy variables are variables that are either 0 or 1. Dummy coding entails assigning one variable the value '0' and the other one value '1'. Often, 0 signifies false/no/none while 1 signifies true/yes. For example, if we wanted to dummy code the nominal variable 'car type' (containing the two categories 'conventional car' and 'electric car'), we could create a column called 'Electric'. We would then set the variable to '0' for cars running on gas and '1' for cars running on electricity.\n",
    "\n",
    "In the IBM dataset we have two variables, which can be dummy coded in this way:\n",
    "- 'Attrition': 'Yes', 'No'\n",
    "- 'Gender': 'Male', 'Female'\n",
    "\n",
    "**Non-ordinal, nominal data** data which hold three or more categories can be handled with *one-hot encoding*. Here, one new binary variable is added for each category in the variable. In other words, one-hot encoding creates one binary variable for each category.\n",
    "\n",
    "Examples of non-ordinal, nominal data in the IBM dataset is:\n",
    "- 'EducationField', 'JobRole', 'MaritalStatus'\n",
    "\n",
    "**One-hot encoding, visualized:**\n",
    "<br>\n",
    "![title](one-hot.png)\n",
    "\n",
    "**Ordinal nominal data** can, in some cases, be replaced by numbers, which signify the natural ordering. For this, the .replace() method can be used.\n",
    "\n",
    "In the IBM dataset, much of the ordinal categories have already been converted into numbers (see the dataset documentation on Kaggle to figure out what the original categories were: https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset/data):\n",
    "- 'Education'\n",
    "- 'JobInvolvement'\n",
    "- 'JobLevel'\n",
    "- 'JobSatisfaction'\n",
    "- 'RelationshipSatisfaction'\n",
    "- 'WorkLifeBalance'\n",
    "- 'PerformanceRating'\n",
    "\n",
    "However, be aware that one need to be careful when replacing categories with numbers. When replacing categories with numbers, we assume a meaningful, sequential ordering of the categories. Moreover, we also assume that the categories are 'evenly spaced'. And, any models we train on this data, will assume this as well.\n",
    "\n",
    "If we look at a variable such as 'Education', which consists of 'bachelor', 'master' and 'phd', these categories are not 'evenly spaced'. A bachelor is usually 3 years (4 in the US), a master is 2 years and a phd 3 years (4 in the us). Thus, the encoding made in the current data (bachelor = 1, master = 2, phd = 3) makes some 'wrong' assumptions, such as: master ('2') is the double of a bachelor ('1') and phd ('3') is three times a bachelor. This might be a problem when training your model. \n",
    "\n",
    "Another, more meaninful, way of encoding the 'Education' variable, could be to code the years of education:\n",
    "- bachelor = '4', signifying that the degree takes 4 years of education (as the data is from the US)\n",
    "- master = '6', signifying that the degree takes 6 years of education (4 from the bachelor + 2 from the master)\n",
    "- phd = '10', signifying that the degree takes 10 years of education (4 from the bachelor + 2 from the master + 4 from the phd)\n",
    "\n",
    "Thus, we can critisize the people who encoded the IBM dataset variable 'Education' and argue that they should either code it based om years spent on education OR one-hot encode it instead.\n",
    "\n",
    "We highlight this, because you will need to make this type of decisions when handling the nominal/categorical data in your own dataset. We encourage you to experiment with your nominal encoding and see how it affects the models you train.\n",
    "\n",
    "Below, we use .replace() to handle 'Attrition', 'Gender' and 'BusinessTravel'.\n",
    "\n",
    "Next, we handle 'EducationField', 'JobRole', 'MaritalStatus' using one-hot encoding (usig the method .get_dummies()).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace({\"Attrition\" : {\"No\": 0, \"Yes\" : 1}}, inplace=True)\n",
    "df.replace({\"Gender\" : {\"Female\": 0, \"Male\" : 1}}, inplace=True)\n",
    "df.replace({\"OverTime\" : {\"No\": 0, \"Yes\" : 1}}, inplace=True)\n",
    "df.replace({\"BusinessTravel\" : {\"Non-Travel\": 0, \"Travel_Rarely\": 1, \"Travel_Frequently\" : 2}}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns = [\"EducationField\", \"Department\", \"JobRole\", \"MaritalStatus\"], dtype='int') \n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which columns our dataset currently holds (after we handled the nominal variables)\n",
    "df.columns\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to take a look at the dataset, post encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see all columns, we first ask Pandas to display all columns:\n",
    "pd.set_option('display.max_columns', None)   \n",
    "\n",
    "# Next, we take a look at the transformed dataset\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can se, we have now succesfully transformed the columns with nominal data.\n",
    "The columns now all contain numbers. We can confirm this by calling .dtypes. As the output shows, all columns now contain integer values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Handling missing values\n",
    "\n",
    "Besides from handling nominal values, data cleaning also consists of finding and handling missing values. For more on how to do this, take a lok at the notebook from week 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## 6. Pearson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little recap from week 3 - BDP_Classification.ipynb.\n",
    "\n",
    "In order to build any model we must specify a **target variable/dependent variable** and one or more **feature variables/independent variables**.\n",
    "\n",
    "- **(X)** = feature or independent variable\n",
    "- **(y)** = target or dependent variable - in this case, 'Attrition'\n",
    "\n",
    "In order to figure out what variables to choose as feature(s), aka, which variables we will use to try to predict our target, we create a **correlation matrix**. \n",
    "\n",
    "The correlation matrix we will create visualizes the **pearson correlation** between the variables in the dataset. Other correlation methods include: \n",
    "- Spearman \n",
    "- Kendall \n",
    "- Gamma \n",
    "- Eta \n",
    "- **You can look up the different correlation types and how they differ on the internet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `corr`: The computed correlation matrix\n",
    "# Method = pearson\n",
    "# Numeric values only. A correlation cannot take into account qualitative data unless they are transformed, which we just did above!\n",
    "corr = df.corr(method='pearson', numeric_only=True)\n",
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Heatmap\n",
    "\n",
    "We have constructed a pearson correlation matrix, however for better readability we will create a **heatmap** based on our correlation matrix. A heatmap is basically a graphical representation of the data that uses color coding to represent different values. \n",
    "\n",
    "\n",
    "Here we use the library Seaborn which we defined as 'sns'.\n",
    "\n",
    "**<font color=Orchid>REMEMBER: If you'd like to inspect the heatmap using zoom, hover your mouse over the plot and click on the middle button (the one looking like a small histogram) in the upper right corner.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a new figure (fig) and axis (ax) with a specified size.\n",
    "fig, ax = plt.subplots(figsize=(50,50)) # Adjust the figsize values to change the size of each cell in the heatmap.\n",
    "\n",
    "# - `xticklabels` & `yticklabels`: Use the columns of the correlation matrix for labeling the x and y axis respectively.\n",
    "#   blue represents positive correlations and red represents negative correlations.\n",
    "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, cmap='RdBu_r', annot=True, linewidth=0.5, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pearson correlation coefficient measures the linear relationship between two variables.\n",
    "Its value ranges from -1 to 1, where:\n",
    "- **1** implies a perfect **positive linear relationship**. If one variable increases, the other variable increases as well\n",
    "- **-1** implies a perfect **negative linear relationship**. When one variable increases, the other variables decreases\n",
    "- **0** implies **no relationship** between the variables\n",
    "- The darker the colour, the more correlated the variables are\n",
    "\n",
    "<font color=Orchid>OBS! Remember that correlation is not causation!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Sorting the heatmap\n",
    "\n",
    "Since we only want to focus on attrition, lets us sort our axes with their correlation with attrition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting the axes by thier correlation with attrition\n",
    "\n",
    "col = df.corr().nlargest(48, \"Attrition\").Attrition.index\n",
    "plt.figure(figsize=(50, 50))\n",
    "sns.heatmap(df[col].corr(), vmin=-0.3, vmax=0.3, cmap='RdBu_r', annot=True, linewidth=0.5, annot_kws={\"size\":10}, fmt='.1g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **<font color= turquoise>EXERCISE 2</font>**\n",
    "\n",
    "What can we conclude from this correlation matrix?\n",
    "And how may the information in the matrix help us in our quest to predict attrition?\n",
    "\n",
    "**Hints** \n",
    "- 'Attrition': 0 = no, the employee did not leave, 1 = yes the employee left\n",
    "- Red signifies a positive correlation = attrition is positively correlated with the given feature\n",
    "- Blue signifies a negative correlation = attrition is negatively correlated with the given feature\n",
    "- The darker the color, the more strongly correlated the variables are "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## 7. Distribution & Visualisation\n",
    "\n",
    "We will now look at the distribution within the different features and visualise them based on Victoria Cox's chapter. \n",
    "\n",
    "We will do through \"basic visualisations\" - but we urge you to do some research and find other types of plots and visualisations. Pairplots, barplots, boxplots, etc...\n",
    "\n",
    "### 7.1 Age column\n",
    "\n",
    "Below, we count the number of employees of different ages. For instance, we can see that there is 8 employees in the dataset, who are 18 years old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cheap way to check consistency and distribution within a column\n",
    "df.groupby(['Age']).Age.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we visualize the age distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].value_counts().sort_index().plot(kind='bar', rot=0, ylabel='count', figsize=(15,5))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# You can also write a for-loop to view all the distributions of the columns without writing out all the colum names\n",
    "#for col in df:\n",
    "#    df[col].value_counts().sort_index().plot(kind='bar', rot=0, ylabel='count')\n",
    "#    plt.show()\n",
    "#Only a requirement outside of Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention to any odd distribution - they can say a lot about the data quality, and *hint hint, missing values!*\n",
    "\n",
    "### **<font color=turquoise>EXERCISE 3</font>**\n",
    "\n",
    "Is there anything odd about the 'Age' distribution? What about other features? Any odd distributions there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Attrition Column\n",
    "\n",
    "Now you know how to visualise and analyse the distribution of a features. It's also important to see the distribution of our target feature - Attrition.\n",
    "\n",
    "*Little recap from Evaluation week - BDP_Evaluation.ipynb*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To better see the number of 'Yes' and 'No' values\n",
    "df.groupby(['Attrition']).Attrition.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualise the distribution of Attrition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Attrition'].value_counts().sort_index().plot(kind='bar', rot=0, ylabel='count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 237 employees have somehow left the company. \n",
    "\n",
    "237 ÷ 1233 = 0.1922 = 19.22%\n",
    "\n",
    "This means that we are working with an imbalanced dataset, and during our modelling phase, we have to have that in mind and take the imbalance into account i.e. when choosing a correct metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 8. Outliers\n",
    "\n",
    "Outliers (i.e., extreme values like '155' in an 'Age'-column) can be wrong values in a dataset. Either due to a typo, conversions, or just due to the context of the dataset.\n",
    "\n",
    "One way to look for outliers, is to look at the min and max value of a given feature/column. Do they make sense in the context of the database and in our real world? Remember! These are assumptions! \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color= turquoise>EXERCISE 4</font>**\n",
    "Can you find the age outliers? Outliers can mess with the result of our model. A way to deal with them is to transform them - either by inserting another value or by removing them from the dataset entirely. \n",
    "\n",
    "*Hint hint!* Go back to the Notebook from week 02 - BDP_Basics, to see how to remove a spefic row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color= turquoise>EXERCISE 5</font>**\n",
    "What are the consequences of transforming an outlier or completely removing it? How does it affect modelling? Are there any ethical concerns doing this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 9. Which variables have the biggest impact?\n",
    "\n",
    "We know from our Pearson Correlation Matrix and Pearson Correlation Heatmap that some features are more strongly correlated to our target feature than others. Obs! Again, remember that correlation is not causation! We need to figure out which features actually have the most impact on 'Attrition', because it is these features that can help us better predict Attrition.\n",
    "\n",
    "We will now create a histogram over the Pearson Correlation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a histogram, but we don't want Attrition there, since that is our target and it would just be correlated with it self.\n",
    "df.drop('Attrition', axis=1).corrwith(df.Attrition).sort_values().plot(kind='barh', figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these variables have a correlation with attrition -- either positive or negative. The variables in the middle, not as much. Therefore, we can start by training a model on  cut the features that have the strongest correlation to attrition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color= turquoise>EXERCISE 6</font>**\n",
    "Examine whether more or less features is better when training classification models using machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "***\n",
    "\n",
    "# Take home messages\n",
    "\n",
    "After finishing this notebook, you should know how to:\n",
    "- thoroughly examine a dataset\n",
    "- encode values\n",
    "- modify a Pearson Correlation heatmap\n",
    "- visualise and analyse a distribution\n",
    "- Find and handle outliers\n",
    "- Decide on which features to use when predicting your target variable\n",
    "\n",
    "## Time to explore and clean your own datasets!\n",
    "\n",
    "## Good luck with your projects!\n",
    "## - Your BDP TAs  ❤"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
