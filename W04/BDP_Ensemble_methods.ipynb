{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c3668c",
   "metadata": {},
   "source": [
    "# Big Data Processes Exercises - Week 04, <b>*part 2*</b>\n",
    "# <font color= Pink> Ensemble methods </font>\n",
    "\n",
    "#### What we will cover today\n",
    "\n",
    "<ol>\n",
    "    <li>Importing packages and libraries</li>\n",
    "    <li>Loading the dataset</li>\n",
    "    <li>Selecting target features</li>\n",
    "    <li>Splitting and Scaling</li>\n",
    "    <li>Bagging</li>\n",
    "    <ol>\n",
    "        <li>Random Forest</li>\n",
    "    </ol>\n",
    "    <li>Boosting</li>\n",
    "    <ol>\n",
    "        <li>AdaBoost</li>\n",
    "        <li>XGBoost</li>\n",
    "    </ol>\n",
    "    <li>Emsemble voting</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13011f3",
   "metadata": {},
   "source": [
    "### What is ensemble methods?\n",
    "\n",
    "The ensemble model technique uses multiple machine learning models for a better result. This can either be by combining them, or stacking them on top of each other.\n",
    "\n",
    "In our case, we want to predict attrition using the IBM dataset using ensemble methods. Thus, we first train several models using our training data. Next, when we want to make a prediction, we run the models using our test data. As the models are different, they might also make different predictions. One model may predict that an employee will leave the company (= attrition), while another model say's the employee won't leave (= no attrition). How do we decide which prediction to stick with? We simple pick the majority vote. In other words, if we have three models and two of our models conclude that the employee will leave, we pick this option.\n",
    "\n",
    "\n",
    "### TA Tip:\n",
    "\n",
    "Think of ensemble methods as \"wisdom of the crowd\". It refers to the case where the opinion calculated from the (the sum) of a group of people is often more accurate, useful, or correct than the opinion of any individual in the group.\n",
    "\n",
    "***\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29006816",
   "metadata": {},
   "source": [
    "## 1. Importing various libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a64e6ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries to work with the data object\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# libraries to visualize\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import export_graphviz\n",
    "from six import StringIO\n",
    "from IPython.display import Image\n",
    "\n",
    "import graphviz\n",
    "import pydotplus\n",
    "\n",
    "# sklearn packages for Decision Tree, KNN, RandomForest and Logistic Regression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cf7c7a",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## 2. Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e40762e",
   "metadata": {},
   "source": [
    "In this notebook, we will be using our 'good old' IBM-employee-attrition dataset, so let's just load this into our notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a88f50eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"IBM-Employee-Attrition.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0544a767",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## 3. Selecting target features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6827bd90",
   "metadata": {},
   "source": [
    "We will select the same features as we've used before for our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16a993ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the feature and target variables\n",
    "#From list of feature(s) 'X', the model will guess/predict the 'y' feature (our target)\n",
    "X = df[['EnvironmentSatisfaction', 'JobSatisfaction', 'JobInvolvement', 'YearsAtCompany', 'StockOptionLevel', 'YearsWithCurrManager', 'Age', 'MonthlyIncome', 'YearsInCurrentRole', 'JobLevel', 'TotalWorkingYears']].values\n",
    "\n",
    "y = df['Attrition'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3afc0c8",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## 4. Splitting and Scaling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc150cf",
   "metadata": {},
   "source": [
    "We split the data as usual, such that we have some data to test the accuracy of our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06aed791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into test and train - 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e39792b",
   "metadata": {},
   "source": [
    "We also scale our data, as we will be using KNN for classification further down and this classification model requires that we scale the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f805bfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a standard scaler object and fit it to the training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform the training and test data using the scaler\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdbd489",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 5. Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8d0ecd",
   "metadata": {},
   "source": [
    "![title](Bagging_Boosting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdbe273",
   "metadata": {},
   "source": [
    "Bagging and boosting are ensemble learning techniques that we can use to improve the performance of our classification models. In this section, we look at bagging. We will get to boosting in the following section. However, both techniques entail training so-called base models (also called 'weak learners') of the same type using our training data. In the image above, the base models are pictured as small robots: <img src=\"https://em-content.zobj.net/source/google/387/robot_1f916.png\" width=\"20\"/> <img src=\"https://em-content.zobj.net/source/google/387/robot_1f916.png\" width=\"20\"/> <img src=\"https://em-content.zobj.net/source/google/387/robot_1f916.png\" width=\"20\"/>\n",
    "\n",
    "\n",
    "In bagging, we train several base models of the same type on the same training data. More specifically, we take *samples* from our training data and train each individual model on a subsample. In the illustration above, you can see how subsamples (the purple sheets) are taken from the original training data (the green sheet). The robots represent classification base models. Each robot is trained on its own subset. Next, we provide the robots with our test data and ask them to make a prediction. The bagging ensemble model then choose the prediction made by the majority of the robots/models. For instance, robot A and B might predict that an employee will stay, while robot C predicts that he will leave. In this case, the bagging ensemble model will predict that the empoyee stays.\n",
    "\n",
    "Last week, you learned about two types of classification models: decision trees and KNN.\n",
    "\n",
    "If we want to use several decision trees in our bagging ensemble model, we can use what is called **random forests**, see section 5.1.\n",
    "\n",
    "(Optional: read more about this classifier here, https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "We can also use another type of model, for instance KNN, see section 5.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f57d991",
   "metadata": {},
   "source": [
    "### 5.1 Random Forrest\n",
    "\n",
    "Random forrest is a subset of Bagging. Again, it is applying multiple methods on subset of data.\n",
    "\n",
    "The fundamental difference between bagging and random forests is that in random forests only a subset of features are selected at random out of the total. Then, the best split feature from the subset is used to split each node in a tree, unlike in bagging where all features are considered for splitting a node. \n",
    "-- Stackoverflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8859e164",
   "metadata": {},
   "source": [
    "When making a random forest, we start by creating a random forest object, which we choose to call 'RF_classifier'. \n",
    "\n",
    "The **n_estimators** parameter defines the number of trees in the forest. In other words, here we state the number of decision trees (= robots) we want to train. If we do not specify anything, the default value will be 100. We limit this number in order to prevent overfitting. \n",
    "\n",
    "We also specify the maximum depth of the tree (tip: we explain this and other relevant parameters for decision trees in the notebook on hyperparameter tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "719e63ff",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# making a random forest object and deciding on 5 decesion trees of depth 5\n",
    "RF_classifier = RandomForestClassifier(n_estimators = 5, max_depth = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d21b18e",
   "metadata": {},
   "source": [
    "Next, we train your model object on our training data, that we created in section 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60af263b",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=5, n_estimators=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=5, n_estimators=5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=5, n_estimators=5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd69308",
   "metadata": {},
   "source": [
    "Then, we test our new random forest classification model on our test data, i.e., we ask the model to predict attrition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a01d89e",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = RF_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed66d3f7",
   "metadata": {},
   "source": [
    "Finally, we measure the accuracy of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57af8336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8605442176870748\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4051e382",
   "metadata": {},
   "source": [
    "As you can see, our random forest model was correct approximatly 86% of the time (N.B.: because of randomness built into the model, the accuracy may vary a little bit from time to time when running the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784930a7",
   "metadata": {},
   "source": [
    "### 5.2 Bagging classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5806cad3",
   "metadata": {},
   "source": [
    "Okay, now lets try to make a bagging ensemble model using KNN models instead of decision trees! \n",
    "\n",
    "Again, we start by making an object of the bagging classifier, telling it use KNN. We also provide the **n_estimators** parameter. This signifies the number of estimators/robots we want the bagging classifier to use (in our case, the number of KNN models). Increasing the number of estimators generally improves performance but also increases computational cost, so it's a trade-off.\n",
    "\n",
    "However, you can provide additional parameters, such as max_samples and max_features.\n",
    "\n",
    "- **max_samples** is the number of samples to draw from X in order to train each decision tree (base estimator) \n",
    "\n",
    "- **max_features** is the number of features to draw from X in order to train each decision tree (base estimator) \n",
    "\n",
    "\n",
    "(Optional: Learn more about the bagging classifier in the documentation, https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe2a21c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BG_KNN_classifier = BaggingClassifier(KNeighborsClassifier(), n_estimators = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d75d98",
   "metadata": {},
   "source": [
    "Next, we train your model object on our training data, that we created in section 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4559ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BaggingClassifier(estimator=KNeighborsClassifier(), n_estimators=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BaggingClassifier</label><div class=\"sk-toggleable__content\"><pre>BaggingClassifier(estimator=KNeighborsClassifier(), n_estimators=100)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BaggingClassifier(estimator=KNeighborsClassifier(), n_estimators=100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BG_KNN_classifier.fit(X_train_std,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49363f52",
   "metadata": {},
   "source": [
    "Finally, we measure the accuracy of the model. Again, the acuracy will vary a bit from time to time due to randomness in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34ccb518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model's accuracy is: \t 0.8673\n"
     ]
    }
   ],
   "source": [
    "accuracy_BG = round(BG_KNN_classifier.score(X_test_std,y_test),4)\n",
    "print(\"The model's accuracy is: \\t\", accuracy_BG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0124b70",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## 6. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e767b0",
   "metadata": {},
   "source": [
    "![title](Bagging_Boosting.png)\n",
    "\n",
    "Boosting is another ensemble learning technique, which we can use to improve the performance of our classification models. Unlike bagging, boosting involves *sequentially* training a series of base models/weak learners/robots, also of the same type (for instance decision trees). Here, each subsequent base model corrects the errors made by the previous ones. Thus, models at each iteration are based on the performance of the previous models. It does so by assigning weights to the training data points in the data - that's why the sheets in the image have different colors. When the boosting model makes a prediction, this preditction is a weighted combination of the predictions made by all the base models, where the weights are determined during the boosting process.\n",
    "\n",
    "(Optional: read more here, https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)\n",
    "\n",
    "Now, we will go through two methods for boosting: AdaBoost and XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5763debf",
   "metadata": {},
   "source": [
    "### 6.1 AdaBoost\n",
    "\n",
    "AdaBoost stands for Adaptive Boosting, is best for binary (two-class)  classification. \n",
    "\n",
    "The parameters are the same just like in Bagging:\n",
    "\n",
    "- **max_samples** is the number of samples to draw from X in order to train each decision tree (base estimator) \n",
    "\n",
    "- **max_features** is the number of features to draw from X in order to train each decision tree (base estimator) \n",
    "\n",
    "- **n_estimators** the number of decision trees(base estimators) in the ensemble \n",
    "\n",
    "Documentation:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17d40391",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# First we create an object from the model's class.\n",
    "model_BO = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 2), n_estimators = 2, learning_rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9bf5db9",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=2),\n",
       "                   learning_rate=1, n_estimators=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=2),\n",
       "                   learning_rate=1, n_estimators=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=2)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=2)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=2),\n",
       "                   learning_rate=1, n_estimators=2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We fit/train our  model on our  traning data, which are the feature vectors (X_train) and the target vector (y_train)\n",
    "model_BO.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9998571",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  We make prediction on the new/unseen test dataset\n",
    "y_pred4 = model_BO.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fc6c167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 252\n",
      "Incorrect predictions: 42\n",
      "Total predictions: 294\n"
     ]
    }
   ],
   "source": [
    "# All your burning questions about the confusion matrix will have to wait until week 5:)) \n",
    "# We use it here to illustrate the number of correct and incorrect predictions\n",
    "mtr = confusion_matrix(y_test, y_pred4)\n",
    "\n",
    "print(\"Correct predictions:\", (mtr[0,0] + mtr[1,1]))\n",
    "print(\"Incorrect predictions:\", (mtr[0,1] + mtr[1,0]))\n",
    "print(\"Total predictions:\", (mtr.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687a5928",
   "metadata": {},
   "source": [
    "### 6.2 XGBoost\n",
    "\n",
    "XGBoost is short for **Extreme Gradient Boosting** is an effective machine learning model, even on datasets where the class distribution is skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e60ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install xgboost\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99945726",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic')\n",
    "# fit model\n",
    "bst.fit(X_train, y_train)\n",
    "# make predictions\n",
    "preds = bst.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8f76e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8537414965986394\n",
      "Correct predictions: 251\n",
      "Incorrect predictions: 43\n",
      "Total predictions: 294\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, preds))\n",
    "# All your burning questions about the confusion matrix will have to wait until week 5:)) \n",
    "# We use it here to illustrate the number of correct and incorrect predictions\n",
    "mtr = confusion_matrix(y_test, preds)\n",
    "\n",
    "print(\"Correct predictions:\", (mtr[0,0] + mtr[1,1]))\n",
    "print(\"Incorrect predictions:\", (mtr[0,1] + mtr[1,0]))\n",
    "print(\"Total predictions:\", (mtr.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bb51172",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain_clf = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "dtest_clf = xgb.DMatrix(X_test, y_test, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae42a570",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 7. Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfced6a",
   "metadata": {},
   "source": [
    "As you now know, bagging and boosting involves training several base models and using them all to make a prediction. However, you can also use multiple base models *of different types* in the same ensemble.\n",
    "\n",
    "In this section we will look at how you create an ensemble consisting of three different types of classification models:\n",
    "- Decision tree\n",
    "- KNN\n",
    "- Logistic regression\n",
    "\n",
    "By using the VotingClassifier, it will aggregate each models' predictions \n",
    "\n",
    "(Optional: read more here, https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c0cda2",
   "metadata": {},
   "source": [
    "### 7.1 Making an ensemble model using the VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8328eb",
   "metadata": {},
   "source": [
    "Our first step is to make an object of each the three base models, we want to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d16120f",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#OBS! Even though  it's called Logistic Regression,  it is atucally a classification technique\n",
    "model_LR = LogisticRegression(solver='liblinear')\n",
    "model_KNN = KNeighborsClassifier()\n",
    "model_DT = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c5b079",
   "metadata": {},
   "source": [
    "Then, we 'load' the models into a voting classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "962d9a2d",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# You can either do hard voting, or soft voting. \n",
    "# In hard voting, we combine the outputs by returning the mode - the most frequently occurring label among the base classifiers’ outputs.\n",
    "# In soft voting, the base classifiers output probabilities or numerical scores.\n",
    "VC = VotingClassifier(estimators= [(\"model_LR\",model_LR),(\"model_KNN\", model_KNN),(\"model_DT\", model_DT)], voting = 'hard')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c7ff9e",
   "metadata": {},
   "source": [
    "Then we train the voting classifier model on our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2dc6170c",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;model_LR&#x27;,\n",
       "                              LogisticRegression(solver=&#x27;liblinear&#x27;)),\n",
       "                             (&#x27;model_KNN&#x27;, KNeighborsClassifier()),\n",
       "                             (&#x27;model_DT&#x27;, DecisionTreeClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;model_LR&#x27;,\n",
       "                              LogisticRegression(solver=&#x27;liblinear&#x27;)),\n",
       "                             (&#x27;model_KNN&#x27;, KNeighborsClassifier()),\n",
       "                             (&#x27;model_DT&#x27;, DecisionTreeClassifier())])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>model_LR</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>model_KNN</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>model_DT</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('model_LR',\n",
       "                              LogisticRegression(solver='liblinear')),\n",
       "                             ('model_KNN', KNeighborsClassifier()),\n",
       "                             ('model_DT', DecisionTreeClassifier())])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VC.fit(X_train_std,  y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e64134",
   "metadata": {},
   "source": [
    "Finally, we test our model on our test data and assess its accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e299b058",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_VC = VC.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68fce3b6",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model's accuracy is: \t 0.8843537414965986\n"
     ]
    }
   ],
   "source": [
    "accuracy_VC = accuracy_score(y_test, y_pred_VC)\n",
    "print(\"The model's accuracy is: \\t\", accuracy_VC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ecc577",
   "metadata": {},
   "source": [
    "### 7.2 Understanding the results of the VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3570ce2",
   "metadata": {},
   "source": [
    "One problem with voting is that it is not clear which classifier to trust. Therefore we will fit the majority rule classifier:\n",
    "\n",
    "\n",
    "Using the **.transform**() method, we can get the prediction for each classifier. If we put the final prediction of the voting process and the actual target labels, we get a really nice overview of our classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f3aa436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87 (+/- 0.02) [Logistic Regression]\n",
      "Accuracy: 0.86 (+/- 0.02) [Random Forest]\n",
      "Accuracy: 0.84 (+/- 0.01) [KNN]\n",
      "Accuracy: 0.87 (+/- 0.01) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "model_LR = LogisticRegression(random_state=1)\n",
    "model_RFC = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "model_KNNC = KNeighborsClassifier()\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', model_LR), ('rf', model_RFC), ('knn', model_KNNC)],voting='hard')\n",
    "\n",
    "for clf, label in zip([model_LR, model_RFC, model_KNNC, eclf], ['Logistic Regression', 'Random Forest', 'KNN', 'Ensemble']):\n",
    "     scores = cross_val_score(clf, X_test_std, y_test, scoring='accuracy', cv=5)\n",
    "     print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9223c35f",
   "metadata": {},
   "source": [
    "We can see that Logistic Regression shows higher accuracy than the rest\n",
    "\n",
    "***\n",
    "***\n",
    "***\n",
    "# Take home messages\n",
    "\n",
    "After finishing this notebook, you should know:\n",
    "- How to use mutliple models in order to perform bagging\n",
    "- How to use boosting\n",
    "- Using multiple models in order to perform voting "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
