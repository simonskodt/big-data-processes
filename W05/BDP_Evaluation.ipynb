{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Processes Exercises - Week 05\n",
    "# <font color= Pink>Evaluation</font>\n",
    "\n",
    "#### What we will cover today\n",
    "\n",
    "<ol>\n",
    "    <li>Importing packages and libraries</li>\n",
    "    <li>Loading the dataset</li>\n",
    "    <li>Selecting target features</li>\n",
    "    <li>Splitting the data</li>\n",
    "    <li>1st Modeling</li>\n",
    "    <li>Evaluation</li>\n",
    "    <ol>\n",
    "        <li>Confusion Matrix</li>\n",
    "        <li>Scores & Metrics</li>\n",
    "    </ol>\n",
    "    <li>Oversampling</li>\n",
    "    <ol>\n",
    "        <li>Random Oversampling</li>\n",
    "        <li>SMOTE</li>\n",
    "    </ol>\n",
    "    <li>Undersamling</li>\n",
    "    <ol>\n",
    "        <li>Random Undersampling</li>\n",
    "        <li>TomekLinks</li>\n",
    "    </ol>\n",
    "    <li>Combination of over- & undersampling</li>\n",
    "    <ol>\n",
    "        <li>SMOTE & TomekLinks</li>\n",
    "    </ol>\n",
    "    <li>Evaluation</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Evaluation Metrics - look up\n",
    "A 'quick' recap from Bruce & Bruce and [machinelearningmastery.com](https://machinelearningmastery.com/metrics-evaluate-machine-learning-algorithms-python//):\n",
    "\n",
    "**<font color= Orchid>Accuracy</font>**<br>\n",
    "The percent (or proportion) of cases classified correctly. Being the most common evaluation metric for classification problems, it is also the most misused. It is really only suitable when there are an equal number of observations in each class. Why is this? Think of a dataset on sunny days in Hawaii. The distribution of sunny vs. not-sunny days would probably be quite unbalanced (e.g., 95% sunny days and 5% not-sunny days). A classifier model trying to predict whether a day will be sunny could receive a very high accuracy by simply always predicting the biggest class (sunny). As such, the accuracy metric does not reflect the model's ability to correctly classificy the smallest class (not-sunny days).\n",
    "\n",
    "Furthermore, the accuracy metric 'assumes' that all predictions and prediction errors are equally important. What do we mean by this? Imagine that we are trying to predict a medical condition. Here, we might have a model which is overall pretty accurate. However, when we take a closer look at the model, we see that there is a lot of 'false negatives' (i.e., patients being misclassified as healthy although they actually have the diagnosis). This misclassification is much more critical than a 'false positive' (i.e., a patient being classified as having the condition, although they are healthy). The accuracy metric itself does not account for this, but treats all false predictions equally. However, as the example shows, certain types of errors might be more costly or harmful than others. The metrics below are ways to address the limitations of the accuracy metric.\n",
    "\n",
    "**<font color= Orchid>Confusion matrix</font>**<br>\n",
    "A tabular display (2Ã—2 in the binary case) of the record counts by their predicted and actual classification status. In other words, it shows the amount of true positive and true negative predictions as well as false positive and false negative predictions made by the model. What does these terms mean? Think of our IBM classification model predicting attrition (i.e. whether an employee will leave the company). In this dataset, the value '1' means 'true' = the employee did leave the company, while the value '0' means 'false' = the employee did not leave the company:\n",
    "- We say a prediction is a *<font color= Orchid>true positive</font>* when the model predicts '1' and the correct class is indeed '1' (the employee actually left the company). \n",
    "- We say a prediction is a *<font color= Orchid>true negative</font>*, when the model predicts '0' and the correct class is indeed '0' (the employee did not leave). \n",
    "- Similarly, a *<font color= Orchid>false positive</font>* prediction is when the model predicts '1', when the correct class is actually '0' (i.e., the employee is predicted to have left the company, but in fact did not leave). \n",
    "- A *<font color= Orchid>false negative</font>* prediction is when the model predicts '0', when the correct class is actually '1' (i.e., the employee is predicted to stay, but in fact left the comapny).\n",
    "\n",
    "**<font color= Orchid>Recall</font>** - minimizing false negatives <br>\n",
    "This metric is also called sensitivity or 'true positive rate'. It signifies the percent (or proportion) of all '1's that are correctly classified as '1's. In other words, it answers the question: <font color= Orchid>*\"Of all the instances in the test data that belong to '1', how many did the model correctly predict as '1'?\"</font>*. In the case of the IBM dataset, we take all the employees that left the company and calculate how many the model predicted correctly as having left the company.\n",
    "\n",
    "Recall is particularly useful in situations where the cost of missing a positive instance (i.e., making a false negative prediction) is high. For example, in medical diagnostics, as described above, recall tells us the percentage of all actual patients with the disease that the model correctly identifies as having the disease. Thus, a high recall means the model is effectively identifying a large portion of patients with a certain condition, minimizing the number of cases where the condition goes undetected. We can optimize our models for high recall, which means that we focus on minimizing false negatives. However, we must be mindful that this may come at the expense of precision (see below), as the model may classify more instances as positive, including some that are actually negative. In other words, a model with high recall may classify more patients as having the condition when they are in fact healthy. \n",
    "\n",
    "**<font color= Orchid>Precision</font>** - minimizing false positives <br>\n",
    "The percent (proportion) of true positives (instances predicted as '1', which are in fact '1') out of all the positive predictions made by the model. It answers the question: *<font color= Orchid>\"Of all the instances in the test data predicted as '1' by the model, how many in this group are in fact '1'?\"*</font>. In the case of the IBM dataset, we take all the employees that was predicted to have left the company and assess how many of these in fact left.\n",
    "\n",
    "Precision helps us to assess whether the positive predictions made by our model are accurate. High precision indicates that the model makes fewer false positive predictions, minimizing the number of instances incorrectly classified as positive. It is important in situations where false positives have high costs or consequences.\n",
    "\n",
    "**<font color= Orchid>Recall vs. Precision</font>**<br>\n",
    "In recall, we first *<font color= Orchid>group together all the positive instances</font>* in our test dataset and then, we figure out how many of these have been correctly predicted as positive by our model. (Recall: How many of the positive items were identified correctly?)\n",
    "<br>\n",
    "In precision, we first *<font color= Orchid>group together all the instances that our model predicted to be positive</font>* and then, we figure out how many of these were in fact positive. (Precision: How many of the items predicted to be positive are actually positive?)\n",
    "\n",
    "\n",
    "**<font color= Orchid>Specificity</font>* <br>\n",
    "The percent (or proportion) of all 0s that are correctly classified as 0s. It measures the ability of a model to correctly identify negative instances (true negatives) out of all actual negative instances.  In other words, specificity answers the question: *<font color= Orchid>\"Of all the instances that truly belong to the negative class, how many did the model correctly predict as negative?\"</font>*. While recall measures the model's ability to identify positive instances, specificity complements this by measuring the model's ability to identify negative instances. A high specificity value indicates that the model is good at avoiding false positives and accurately classifying negative instances. Conversely, a low specificity value suggests that the model is incorrectly classifying negative instances as positive, leading to false alarms. To return to our medical example, if specificity is high, it means the model is good at correctly identifying healthy patients as healthy, minimizing the number of healthy patients mistakenly classified as having a disease.\n",
    "\n",
    "**<font color= Orchid>Area Under ROC curve</font>** <br>\n",
    "Is also called AUC-ROC for short. The AUC-ROC score ranges from 0 to 1 where:\n",
    "- AUC-ROC = 1 indicates a perfect classifier that achieves perfect separation between positive and negative instances.\n",
    "- AUC-ROC = 0.5 indicates a classifier that performs no better than random guessing.\n",
    "- AUC-ROC < 0.5 indicates a classifier that performs worse than random guessing.\n",
    "\n",
    "In other words, it's a number that tells you how well your model can separate positive cases from negative cases overall. The higher the AUC-ROC, the better the model is at this task. If the AUC-ROC is 1, it means the model is perfect. If it's 0.5, it's no better than flipping a coin.\n",
    "It is a great performance metric for binary classification problems.\n",
    "\n",
    "Below, the ROC curve is pictured. The AUC-ROC is like a summary score for this graph.\n",
    "\n",
    "<img src=\"roc.jpg\" width=\"500\"/>\n",
    "\n",
    "**<font color= Orchid>F1 curve</font>** <br>\n",
    "The F1 score is a single metric that combines both precision and recall into one value, providing a balanced measure of a model's performance, especially in situations where there is an imbalance between the classes. In words, the F1 score is the harmonic mean of precision and recall. The harmonic mean gives more weight to low values. This means that the F1 score will be high only if both precision and recall are high.\n",
    "\n",
    "***\n",
    "***\n",
    "*** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Importing various packages and libraries\n",
    "\n",
    "Install the imblearn library if you don't have it yet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "#Sampling libraries\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## 2. Loading dataset and examining the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will again be using the **IBM-employee-attrition dataset** where we will try and predict if an employee has attrition or not, aka. whether they have left the company or not. \n",
    "\n",
    "As we have explained before, our target variable, attrition, is either be 0 or 1:\n",
    "- 0 = No attrition, the employee did not leave the company\n",
    "- 1 = Attrition, the employee left the company\n",
    "\n",
    "Let's load the dataset into our notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"IMB-Employee-Attrition.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the same dataset as last week, but feel free to explore it yet again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## 3. Selecting target features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will select the same features as we've used before for our models (go back to the notebook for week 3, if you need a refresher as to how we got these features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the feature and target variables\n",
    "#From list of feature(s) 'X', the model will guess/predict the 'y' feature (our target)\n",
    "X = df[['EnvironmentSatisfaction', 'JobSatisfaction', 'JobInvolvement', 'YearsAtCompany', 'StockOptionLevel', 'YearsWithCurrManager', 'Age', 'MonthlyIncome', 'YearsInCurrentRole', 'JobLevel', 'TotalWorkingYears']].values\n",
    "\n",
    "y = df['Attrition'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## 4. Splitting the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data as usual, such that we have some data to train the model, and some data to test the accuracy of our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into test and train - 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## 5. 1st Modelling - Ordinary Decision Tree with no sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classificer _ No Sampling\n",
    "DTC_ns = DecisionTreeClassifier(criterion=\"gini\", max_depth=6, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTC_ns.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTC_ns_pred = DTC_ns.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, DTC_ns_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtr = confusion_matrix(y_test, DTC_ns_pred)\n",
    "\n",
    "print(\"Correct predictions:\", (mtr[0,0] + mtr[1,1]))\n",
    "print(\"Incorrect predictions:\", (mtr[0,1] + mtr[1,0]))\n",
    "print(\"Total predictions:\", (mtr.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our decision tree classifier with a max depth of 6 has an accuracy of ~85%. However, as noted in the beginning, the accuracy score alone is not enough to truly evaluate the performance of a model. So, now, let us evaluate this model utilizing our new metrics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## 6. Evaluation\n",
    "\n",
    "In this section, we will be focusing on:\n",
    "- Accuracy score\n",
    "- Confusion matrix\n",
    "- AUC-RUC\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score\n",
    "\n",
    "If you are unsure of what these metrics are, go back to the introductory section of this notebook, where we define the terms.\n",
    "\n",
    "**Accuracy** is calculated by the number of correct predictions divided by the total number of predictions, so:\n",
    "\n",
    "<font color=Orchid> Accuracy = Correct Predictions / Total Predictions </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", metrics.accuracy_score(y_test, DTC_ns_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see  that our model is doing pretty good with ~85% accuracy. But a problem with accuracy within classification is that it hides a lot of the details, which you need know, to better understand the performance of your model. \n",
    "\n",
    "When your data has two or more classes to predict, the accuracy score will never know if the classes are being predicted equally well or whether one or two classes are being neglected by the model based on accuracy.\n",
    "\n",
    "Let us take a look at the class distribution in the IBM dataset, when it comes to Attrition:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Attrition']).Attrition.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 237 of employees (19%) in the dataset left the company. \n",
    "\n",
    "**237 Ã· 1233 = 0.1922 = 19.22%**\n",
    "\n",
    "This means that we are working with an imbalanced dataset. Therefore we cannot use accuracy as a good performance metric. Because our model learned on our training data that there are more 'No  Attrition' instances than 'Attrition' instances, it will always rather predict 'No Attrition', since there is a better chance it will be correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Confusion matrix\n",
    "\n",
    "Since we have now found out that accuracy score alone is not a good metric for the IBM dataset, we will use a confusion matrix to further evaluate our model. As you may recall, the confusion matrix shows the amount of true positive and true negative predictions as well as the amount of false positive and false negative predictions made by the model.\n",
    "\n",
    "The confusion matrix is the basis for many of the evaluation matricies. It provides more insight into not only the performance of the model but also which classes are being predicted correctly, which incorrectly, and what type of errors are being made.\n",
    "\n",
    "Now, let's make a confusion matrix!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_test, DTC_ns_pred)\n",
    "\n",
    "print(\"Confusion matrix for the Decision Tree Classifier with No Sampling on test set: \\n\\n\", cf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix with âœ¨ visualisation âœ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_names = ['True Negative','False Positive','False Negative','True Posistive']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cf_matrix, annot=labels, fmt=\"\", cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **True positive** for correctly predicted target values (correctly predicted 'Attrition')\n",
    "- **False positive** for incorrectly predicted target values (incorrectly predicted 'Attrition')\n",
    "- **True negative** for correctly predicted no-event values (correctly predicted 'No Attrition')\n",
    "- **False negative** for incorrectly predicted no-event values (incorrectly predicted 'No Attrition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color=turquoise>EXERCISE 1</font>**\n",
    "Explain what you can conclude from the confusion matrix visualized above. What does the confusion matrix tell you about the model, that you couldn't see from the accuracy score alone? \n",
    "\n",
    "Also, imagine our model was classifiying patients according to whether they had a diagnosis of some sort. How many patients would have been classified as healthy although they actually had the condition and needed treatment?b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type in your answers here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Scores & Metrics\n",
    "\n",
    "From the confusion  matrix we can calculate other scores that we can use as our metrics.\n",
    "\n",
    "- **Precision**. Calculated by: <font color=Orchid> TruePositive / (TruePositive + FalsePositive)  </font>\n",
    "- **Recall**. Calculated by: <font color=Orchid> TruePositive / (TruePositive + FalseNegative)</font>\n",
    "- **F1 Score**. Calculated by: <font color=Orchid>(2 * Precision * Recall) / (Precision + Recall)</font>\n",
    "- **ROC-AUC**. ROC plots the True Positive rate (TPR) against the False Positive rate (FPR). The ROC-AUC (area under the ROC curve)  then sums up how well a model can produce relative scores to discriminate between positive or negative instances across all classification thresholds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision \n",
    "precision = precision_score(y_test, DTC_ns_pred) \n",
    "# Recall \n",
    "recall = recall_score(y_test, DTC_ns_pred) \n",
    "# F1-Score \n",
    "f1 = f1_score(y_test, DTC_ns_pred) \n",
    "# ROC Curve and AUC \n",
    "fpr, tpr, thresholds = roc_curve(y_test, DTC_ns_pred) \n",
    "roc_auc = auc(fpr, tpr) \n",
    " \n",
    "print(\"Precision:\", precision) \n",
    "print(\"Recall:\", recall) \n",
    "print(\"F1-Score:\", f1) \n",
    "print(\"AUC-ROC:\", roc_auc) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color=turquoise>EXERCISE 2</font>**\n",
    "Revisit the explanations in the introduction of this notebook (or look up the metrics on the internet). Then explain what each of these values tells us about the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your explanations go here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also other metrics out there. If you are confused about which one to use, let this be your guide. \n",
    "\n",
    "    Are you predicting probabilities?\n",
    "        Do you need class labels?\n",
    "            Is the minority class more important?\n",
    "                Use Precision-Recall AUC\n",
    "            Are both classes important?\n",
    "                Use ROC AUC\n",
    "        Do you need probabilities?\n",
    "            Use Brier Score and Brier Skill Score\n",
    "    Are you predicting class labels?\n",
    "        Is the minority class more important?\n",
    "            Are False Negatives and False Positives Equally Important?\n",
    "                Use F1-Measure\n",
    "            Are False Negatives More Important?\n",
    "                Use F2-Measure\n",
    "            Are False Positives More Important?\n",
    "                Use F0.5-Measure\n",
    "        Are both classes important?\n",
    "            Do you have < 80%-90% Examples for the Majority Class? \n",
    "                Use Accuracy\n",
    "            Do you have > 80%-90% Examples for the Majority Class? \n",
    "                Use G-Mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## 7. Data oversampling\n",
    "\n",
    "\n",
    "We will now take a look at data sampling - oversampling in this section and undersampling in the next section. Over- and under-sampling are techniques used in machine learning to address class imbalance in the dataset, where one class is significantly underrepresented compared to the other class. We will only introduce you to two undersampling and two oversampling techniques and a combination of two sampling techniqes - there exist other, maybe even better, techniques, so don't feel constrained by this notebook and feel free to explore om your own!\n",
    "\n",
    "Also, remember these sampling techniques are not models, but techniques to transform your data, in case of an imbalance. After you have transformed your data, you can apply different (classification) models and examine whether these techniques have yielded you better models. \n",
    "\n",
    "Thus, in the coming sections, we will create a decision tree for each newly data sampled dataset. We will then evaluate their results at the end of this notebook.\n",
    "\n",
    "<img src=\"Data_sampling.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section 6 of this notebook, we found that our IBM dataset was imbalanced. More specifically, we saw that only 19% of the employees included in the dataset left the company. To address this issue, we can make use of over-sampling. Over-sampling provides our model with more examples of the minority class. This can help us:\n",
    "- Balance the classes in our dataset\n",
    "- Reduce bias resulting from this imbalance\n",
    "- Potentially improve perforance metrics of the model such as accuracy, precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Random Oversampling\n",
    "\n",
    "Randomly duplicate examples in the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(y_train))\n",
    "ranOveSam = RandomOverSampler(sampling_strategy='minority')\n",
    "X_ro, y_ro = ranOveSam.fit_resample(X_train, y_train)\n",
    "print(Counter(y_ro))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new Decision Tree Classifier, fit the model on our newly random oversampled data, and predict on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Classifier _ Random OverSampling\n",
    "DTC_ros = DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTC_ros.fit(X_ro, y_ro)\n",
    "DTC_ros_pred = DTC_ros.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curious as to how well this new model performs? We will get back to this in sectionn 10 :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 SMOTE Oversampling\n",
    "\n",
    "SMOTE stands for **Synthetic Minority Oversampling TEchnique**. SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample as a point along that line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(y_train))\n",
    "smoOveSam = SMOTE(sampling_strategy=0.5)\n",
    "X_S, y_S = smoOveSam.fit_resample(X_train, y_train)\n",
    "print(Counter(y_S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Classifier _ SMOTE\n",
    "DTC_SMOTE = DecisionTreeClassifier(random_state=42)\n",
    "DTC_SMOTE.fit(X_S, y_S)\n",
    "DTC_SMOTE_pred = DTC_SMOTE.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curious as to how well this new model performs? We will get back to this in sectionn 10 :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## 8. Data Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section 6 of this notebook, we found that our IBM dataset was imbalanced. More specifically, we saw that only 19% of the employees included in the dataset left the company. To address this issue, we first made use of over-sampling (see section above). In this section, we will try out under-sampling.\n",
    "\n",
    "Under-sampling involves reducing the number of instances in the majority class to match the number of instances in the minority class. This can help us:\n",
    "- balance the classes in our dataset\n",
    "- reduce bias resulting from this imbalance\n",
    "- reduce overfitting, especially when the majority class contains a large number of redundant or similar instances\n",
    "- potentially improve perforance metrics of the model such as accuracy, precision and recall\n",
    "<br><br>\n",
    "\n",
    "<img src=\"Data_sampling.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Random Undersampling\n",
    "\n",
    "Randomly delete examples in the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(y_train))\n",
    "ranUndSam = RandomUnderSampler(sampling_strategy='majority')\n",
    "X_ru, y_ru = ranUndSam.fit_resample(X_train, y_train)\n",
    "print(Counter(y_ru))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Classifier _ Random UnderSampling\n",
    "DTC_rus = DecisionTreeClassifier(random_state=42)\n",
    "DTC_rus.fit(X_ru, y_ru)\n",
    "DTC_rus_pred = DTC_rus.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curious as to how well this new model performs? We will get back to this in sectionn 10 :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 TomekLinks Undersampling\n",
    "\n",
    "A Tomekâ€™s link exists when two samples from different classes are closest neighbors to each other. TomekLink() detects and removes the sample of the majority class or both, since the samples are noisy to the dataset and make it harder to classify observation since instances of both classes are closely present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(y_train))\n",
    "tomLinUndSam = TomekLinks(sampling_strategy='majority')\n",
    "X_T, y_T = tomLinUndSam.fit_resample(X_train, y_train)\n",
    "print(Counter(y_T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Classifier _ Tomek UnderSampling\n",
    "DTC_tus = DecisionTreeClassifier(random_state=42)\n",
    "DTC_tus.fit(X_T, y_T)\n",
    "DTC_tus_pred = DTC_tus.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curious as to how well this new model performs? We will get back to this in sectionn 10 :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## 9. Combining Data Sampling techniques\n",
    "\n",
    "You can also combine over- and undersampling techniques for better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Combining Oversampling and Undersampling with SMOTE and TomekLinks\n",
    "\n",
    "Luckily, there already exists a function for combining SMOTE and TomekLinks called **SMOTETomek()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(y_train))\n",
    "resample = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\n",
    "X_ST, y_ST = resample.fit_resample(X_train, y_train)\n",
    "print(Counter(y_ST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Classifier _ SMOTE Tomek\n",
    "DTC_ST = DecisionTreeClassifier(random_state=42)\n",
    "DTC_ST.fit(X_ST, y_ST)\n",
    "DTC_ST_pred = DTC_ST.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curious as to how well this new model performs? We will get to that now! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## 10. Evaluting Data Sampling Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cf_matrix = confusion_matrix(y_test, DTC_ros_pred)\n",
    "# Accuracy \n",
    "accuracy = accuracy_score(y_test, DTC_ros_pred) \n",
    "# Precision \n",
    "precision = precision_score(y_test, DTC_ros_pred) \n",
    "# Recall \n",
    "recall = recall_score(y_test, DTC_ros_pred) \n",
    "# F1-Score \n",
    "f1 = f1_score(y_test, DTC_ros_pred) \n",
    "# ROC Curve and AUC \n",
    "fpr, tpr, thresholds = roc_curve(y_test, DTC_ros_pred) \n",
    "roc_auc = auc(fpr, tpr) \n",
    "\n",
    "print(\"Confusion Matrix:\"'\\n', cf_matrix)\n",
    "print(\"Accuracy:\", accuracy) \n",
    "print(\"Precision:\", precision) \n",
    "print(\"Recall:\", recall) \n",
    "print(\"F1-Score:\", f1) \n",
    "print(\"ROC AUC:\", roc_auc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cf_matrix = confusion_matrix(y_test, DTC_SMOTE_pred)\n",
    "# Accuracy \n",
    "accuracy = accuracy_score(y_test, DTC_SMOTE_pred) \n",
    "# Precision \n",
    "precision = precision_score(y_test, DTC_SMOTE_pred) \n",
    "# Recall \n",
    "recall = recall_score(y_test, DTC_SMOTE_pred) \n",
    "# F1-Score \n",
    "f1 = f1_score(y_test, DTC_SMOTE_pred) \n",
    "# ROC Curve and AUC \n",
    "fpr, tpr, thresholds = roc_curve(y_test, DTC_SMOTE_pred) \n",
    "roc_auc = auc(fpr, tpr) \n",
    "\n",
    "print(\"Confusion Matrix:\"'\\n', cf_matrix)\n",
    "print(\"Accuracy:\", accuracy) \n",
    "print(\"Precision:\", precision) \n",
    "print(\"Recall:\", recall) \n",
    "print(\"F1-Score:\", f1) \n",
    "print(\"ROC AUC:\", roc_auc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cf_matrix = confusion_matrix(y_test, DTC_rus_pred)\n",
    "# Accuracy \n",
    "accuracy = accuracy_score(y_test, DTC_rus_pred) \n",
    "# Precision \n",
    "precision = precision_score(y_test, DTC_rus_pred) \n",
    "# Recall \n",
    "recall = recall_score(y_test, DTC_rus_pred) \n",
    "# F1-Score \n",
    "f1 = f1_score(y_test, DTC_rus_pred) \n",
    "# ROC Curve and AUC \n",
    "fpr, tpr, thresholds = roc_curve(y_test, DTC_rus_pred) \n",
    "roc_auc = auc(fpr, tpr) \n",
    "\n",
    "print(\"Confusion Matrix:\"'\\n', cf_matrix)\n",
    "print(\"Accuracy:\", accuracy) \n",
    "print(\"Precision:\", precision) \n",
    "print(\"Recall:\", recall) \n",
    "print(\"F1-Score:\", f1) \n",
    "print(\"ROC AUC:\", roc_auc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TomekLinks Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cf_matrix = confusion_matrix(y_test, DTC_tus_pred)\n",
    "# Accuracy \n",
    "accuracy = accuracy_score(y_test, DTC_tus_pred) \n",
    "# Precision \n",
    "precision = precision_score(y_test, DTC_tus_pred) \n",
    "# Recall \n",
    "recall = recall_score(y_test, DTC_tus_pred) \n",
    "# F1-Score \n",
    "f1 = f1_score(y_test, DTC_tus_pred) \n",
    "# ROC Curve and AUC \n",
    "fpr, tpr, thresholds = roc_curve(y_test, DTC_tus_pred) \n",
    "roc_auc = auc(fpr, tpr) \n",
    "\n",
    "print(\"Confusion Matrix:\"'\\n', cf_matrix)\n",
    "print(\"Accuracy:\", accuracy) \n",
    "print(\"Precision:\", precision) \n",
    "print(\"Recall:\", recall) \n",
    "print(\"F1-Score:\", f1) \n",
    "print(\"ROC AUC:\", roc_auc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE & TomekLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cf_matrix = confusion_matrix(y_test, DTC_ST_pred)\n",
    "# Accuracy \n",
    "accuracy = accuracy_score(y_test, DTC_ST_pred) \n",
    "# Precision \n",
    "precision = precision_score(y_test, DTC_ST_pred) \n",
    "# Recall \n",
    "recall = recall_score(y_test, DTC_ST_pred) \n",
    "# F1-Score \n",
    "f1 = f1_score(y_test, DTC_ST_pred) \n",
    "# ROC Curve and AUC \n",
    "fpr, tpr, thresholds = roc_curve(y_test, DTC_ST_pred) \n",
    "roc_auc = auc(fpr, tpr) \n",
    "\n",
    "print(\"Confusion Matrix:\"'\\n', cf_matrix)\n",
    "print(\"Accuracy:\", accuracy) \n",
    "print(\"Precision:\", precision) \n",
    "print(\"Recall:\", recall) \n",
    "print(\"F1-Score:\", f1) \n",
    "print(\"ROC AUC:\", roc_auc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these score differ drastically from our ordinary decision tree classifier in sections 5-6. Our new decision trees that have been trained on under- and undersampled data may have a worse accuracy than the ordinary decision tree, however they are more correct in term of what and **how** we want to predict something. \n",
    "\n",
    "We don't want a model that **always** guesses 'No attrition' since that is the majority class and just disgards our other class 'Attrition'. Therefore it is safe to say, that all the models we have build in the previous week are not as great as we thought ðŸ˜„ðŸ˜„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color=turquoise>EXERCISE 3</font>**\n",
    "Try out some of the over- and undersampling techniques we presented above on models from previous weeks (week 3 - classification and week 4 - ensemble methods). Then examine the results - which model performs the best? Did you have to use oversampling or undersampling? Did you do a combination of both data sampling techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "***\n",
    "# Take home messages\n",
    "\n",
    "After finishing this notebook, you should:\n",
    "- be able to perform - and preferably also explain (!) - the evaluation metrices for classification presented in this notebook\n",
    "- be able to perform data over- and under-sampling and explain when and why this is useful\n",
    "- evalute your models based onm more than just accuracy score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
